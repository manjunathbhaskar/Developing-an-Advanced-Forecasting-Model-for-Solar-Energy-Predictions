# -*- coding: utf-8 -*-
"""CEEMDANGRUTNC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QVgSSyNyaPRVG0R7f1uMPDAxsMYokM7o
"""



from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import MinMaxScaler

# Load the data
data_path = '/content/drive/MyDrive/data_inverter_tn.csv'
data = pd.read_csv(data_path)

# Preview the data
print("Initial Data Preview:")
print(data.head())

# Convert the 'sent_date' to datetime format
data['sent_date'] = pd.to_datetime(data['sent_date'])

# Save a copy of the initial data
data_initial_copy = data.copy()

# Separate non-numeric columns
non_numeric_cols = ['sent_date', 'CRC16']
numeric_data = data.drop(columns=non_numeric_cols)

# Clean and convert data types for numeric columns
for column in numeric_data.columns:
    if numeric_data[column].dtype == 'object':
        numeric_data[column] = numeric_data[column].str.replace(',', '')
        numeric_data[column] = pd.to_numeric(numeric_data[column], errors='coerce')

# Save a copy after initial cleaning
data_cleaned_copy = numeric_data.copy()

# Function to process data in batches
def batch_impute(data, batch_size=10000):
    n_batches = len(data) // batch_size + int(len(data) % batch_size != 0)
    imputed_batches = []

    for i in range(n_batches):
        batch = data.iloc[i * batch_size:(i + 1) * batch_size]
        imputer = KNNImputer(n_neighbors=5)
        imputed_batch = imputer.fit_transform(batch)

        # Check the shape of the batch to ensure it matches the expected number of columns
        if imputed_batch.shape[1] != data.shape[1]:
            raise ValueError("Column mismatch after imputation in batch processing.")

        imputed_batches.append(imputed_batch)

        # Print progress
        print(f"Processed batch {i+1}/{n_batches}")

    return np.vstack(imputed_batches)

# Handling missing values using KNN Imputer in batches for numeric data
print("\nHandling missing values in batches...")
data_imputed_numeric = batch_impute(numeric_data)

# Convert back to DataFrame
data_imputed_numeric = pd.DataFrame(data_imputed_numeric, columns=numeric_data.columns)

# Combine imputed numeric data with non-numeric data
data_imputed = pd.concat([data_imputed_numeric, data[non_numeric_cols].reset_index(drop=True)], axis=1)

# Save a copy after imputation
data_imputed_copy = data_imputed.copy()

# Removing Outliers
print("\nRemoving outliers...")
iso_forest = IsolationForest(contamination=0.01)
outliers = iso_forest.fit_predict(data_imputed_numeric)
data_cleaned = data_imputed_numeric[outliers == 1]

# Save a copy after removing outliers
data_cleaned_copy = data_cleaned.copy()

print("\nNormalizing the data...")
scaler = MinMaxScaler()
data_normalized = pd.DataFrame(scaler.fit_transform(data_cleaned), columns=data_cleaned.columns)

data_normalized_copy = data_normalized.copy()

features = data_normalized.columns
lags = [1, 2, 3]
windows = [3, 7, 14]

def create_lag_features(df, lags, column):
    for lag in lags:
        df[f'{column}_lag_{lag}'] = df[column].shift(lag)
    return df

def create_rolling_features(df, windows, column):
    for window in windows:
        df[f'{column}_roll_mean_{window}'] = df[column].rolling(window=window).mean()
        df[f'{column}_roll_std_{window}'] = df[column].rolling(window=window).std()
    return df

print("\nCreating lag and rolling features for the train data...")
for feature in features:
    data_normalized = create_lag_features(data_normalized, lags, feature)
    data_normalized = create_rolling_features(data_normalized, windows, feature)

data_normalized.dropna(inplace=True)

data_feature_engineered_copy = data_normalized.copy()

# Assuming the data has a datetime column named 'sent_date'
data_normalized['sent_date'] = pd.to_datetime(data_imputed['sent_date'])
data_normalized.sort_values('sent_date', inplace=True)

train_data = data_normalized[data_normalized['sent_date'] < '2022-12-01']
test_data = data_normalized[data_normalized['sent_date'] >= '2022-12-01']

train_data_copy = train_data.copy()
test_data_copy = test_data.copy()

train_data = train_data.drop(columns=['sent_date'])
test_data = test_data.drop(columns=['sent_date'])

print("\nTrain Data:")
print(train_data.head())
print("\nTest Data:")
print(test_data.head())

# Add season and time of day columns
def get_season(date):
    month = date.month
    if month in [12, 1, 2]:
        return 'Winter'
    elif month in [3, 4, 5]:
        return 'Spring'
    elif month in [6, 7, 8]:
        return 'Summer'
    else:
        return 'Autumn'

def get_time_of_day(date):
    hour = date.hour
    if 5 <= hour < 12:
        return 'Morning'
    elif 12 <= hour < 17:
        return 'Afternoon'
    elif 17 <= hour < 21:
        return 'Evening'
    else:
        return 'Night'

data_imputed['season'] = data_imputed['sent_date'].apply(get_season)
data_imputed['time_of_day'] = data_imputed['sent_date'].apply(get_time_of_day)

# Save the updated dataset
data_imputed.to_csv('data_imputed_with_season_and_time.csv', index=False)
print("Data with season and time of day columns added and saved.")

import matplotlib.pyplot as plt

# Analysis based on time of day
time_of_day_avg = data_imputed.groupby('time_of_day')['Wh'].mean()
fig, ax = plt.subplots(figsize=(10, 6))
time_of_day_avg.plot(kind='bar', ax=ax, title='Average Energy Output by Time of Day')
ax.set_xlabel('Time of Day')
ax.set_ylabel('Average Wh')
plt.show()

# Analysis based on season
season_avg = data_imputed.groupby('season')['Wh'].mean()
fig, ax = plt.subplots(figsize=(10, 6))
season_avg.plot(kind='bar', ax=ax, title='Average Energy Output by Season')
ax.set_xlabel('Season')
ax.set_ylabel('Average Wh')
plt.show()

# Analyzing energy output at different times of the day
data_imputed['hour'] = pd.to_datetime(data_imputed['sent_date']).dt.hour
hourly_data = data_imputed.groupby('hour')['Wh'].mean()

# Plotting average energy output by hour
fig, ax = plt.subplots(figsize=(10, 6))
hourly_data.plot(kind='bar', ax=ax)
ax.set_title('Average Energy Output by Hour of the Day')
ax.set_xlabel('Hour of the Day')
ax.set_ylabel('Average Wh')
plt.show()

# Further analysis can include average energy output by month
data_imputed['month'] = pd.to_datetime(data_imputed['sent_date']).dt.month
monthly_data = data_imputed.groupby('month')['Wh'].mean()

# Plotting average energy output by month
fig, ax = plt.subplots(figsize=(10, 6))
monthly_data.plot(kind='bar', ax=ax)
ax.set_title('Average Energy Output by Month')
ax.set_xlabel('Month')
ax.set_ylabel('Average Wh')
plt.show()

!pip3 install PyEMD
!pip3 install EMD-signal

!pip3 install keras-tcn

!pip install pandas numpy matplotlib scikit-learn tensorflow PyEMD EMD-signal
!pip install keras-tcn --no-dependencies
!pip install keras-tuner

<<<<<MAIN CODE STARTS FROM HERE>>>>>

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PyEMD import CEEMDAN
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense
from tcn import compiled_tcn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.impute import KNNImputer
from sklearn.ensemble import IsolationForest
from sklearn.ensemble import RandomForestRegressor
import keras_tuner as kt
from tensorflow.keras.optimizers import Adam
import os

def setup_directories(save_dir):
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    print(f"Directory {save_dir} is set up.")

def load_data(data_path):
    print("Loading data...")
    data = pd.read_csv(data_path, parse_dates=['sent_date'], index_col='sent_date')
    data = data.replace({',': ''}, regex=True)
    data = data.apply(pd.to_numeric, errors='coerce')
    print("Data loaded successfully.")
    return data

def filter_data_by_id(data, device_ids):
    print(f"Filtering data for device_ids: {device_ids}")
    filtered_data = data[data['device_id'].isin(device_ids)]
    print(f"Filtered data contains {filtered_data.shape[0]} rows.")
    return filtered_data

# Main function for Part 1
def main_part1():
    save_dir = '/content/drive/MyDrive/thesis/'
    data_path = '/content/drive/MyDrive/data_inverter_tn.csv'

    setup_directories(save_dir)
    data = load_data(data_path)

    # Filter by device IDs
    device_ids = [155, 156]
    filtered_data = filter_data_by_id(data, device_ids)

    return filtered_data, save_dir, device_ids

# Run Part 1
filtered_data, save_dir, device_ids = main_part1()
# Sample 25% of the data
sampled_data = filtered_data.sample(frac=0.25, random_state=42)
print(f"Sampled data contains {sampled_data.shape[0]} rows.")

def load_and_sample_data():
    filtered_data, save_dir, device_ids = main_part1()

    initial_sample = filtered_data.sample(frac=0.10, random_state=42)
    print(f"Initial sampled data contains {initial_sample.shape[0]} rows.")

    # Take 50% of the initial sample
    final_sample = initial_sample.sample(frac=0.5, random_state=42)
    print(f"Final sampled data contains {final_sample.shape[0]} rows.")

    return final_sample, save_dir, device_ids

final_sample, save_dir, device_ids = load_and_sample_data()

device_155_sample = final_sample[final_sample['device_id'] == 155]

print("Sampled Data Preview for Device ID 155:")
print(device_155_sample.head())

print(f"\nSave Directory: {save_dir}")
print(f"Device IDs: {device_ids}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PyEMD import CEEMDAN
import os
import time

# Part 1: Load and Sample Data
def load_and_sample_data():
    filtered_data, save_dir, device_ids = main_part1()

    # Sample a smaller portion of the data to reduce processing time
    initial_sample = filtered_data.sample(frac=0.10, random_state=42)
    print(f"Initial sampled data contains {initial_sample.shape[0]} rows.")

    # Take 50% of the initial sample
    final_sample = initial_sample.sample(frac=0.5, random_state=42)
    print(f"Final sampled data contains {final_sample.shape[0]} rows.")

    return final_sample, save_dir, device_ids

def preprocess_data(data):
    print("Preprocessing data...")
    scaler = MinMaxScaler(feature_range=(0, 1))
    if 'W' in data.columns:
        data['W'] = scaler.fit_transform(data[['W']].values.reshape(-1, 1)).flatten()
        print("Data preprocessing completed.")
    else:
        raise ValueError("Column 'W' is missing in the data.")
    return data, scaler

def detect_and_remove_outliers(data, feature='W'):
    print("Detecting and removing outliers...")
    iso_forest = IsolationForest(contamination=0.01, random_state=42, n_jobs=-1)
    iso_forest.fit(data[[feature]])
    outliers = iso_forest.predict(data[[feature]])
    data['outlier'] = outliers
    clean_data = data[data['outlier'] == 1].drop('outlier', axis=1)
    print("Outliers removed.")
    return clean_data

def impute_missing_values(data):
    print("Imputing missing values...")
    imputer = KNNImputer(n_neighbors=5)
    data_filled = pd.DataFrame(imputer.fit_transform(data), columns=data.columns, index=data.index)
    print("Missing values imputed.")
    return data_filled

def decompose_series(data, save_dir, title='Decomposed IMFs', num_imfs=8):
    print("Decomposing time series...")
    ceemdan = CEEMDAN()

    imfs = ceemdan(data)

    imfs = imfs[:num_imfs]

    residual = data - np.sum(imfs, axis=0)

    # Save IMFs and residual
    np.save(os.path.join(save_dir, f'{title}_imfs.npy'), imfs)
    np.save(os.path.join(save_dir, f'{title}_residual.npy'), residual)

    plot_imfs_and_residual(imfs, residual, title, save_dir)
    print("Time series decomposed, IMFs and residual saved.")
    return imfs, residual

def plot_imfs_and_residual(imfs, residual, title, save_dir):
    plt.figure(figsize=(15, 12))
    num_imfs = len(imfs)

    for i, imf in enumerate(imfs):
        plt.subplot(num_imfs + 1, 1, i + 1)
        plt.plot(imf)
        plt.title(f'IMF {i + 1}')

    plt.subplot(num_imfs + 1, 1, num_imfs + 1)
    plt.plot(residual)
    plt.title('Residual')

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, f'{title}.png'))
    plt.close()
    print(f"IMFs and residual plotted and saved as {title}.png.")

def preprocess_and_decompose_one_device(filtered_data, save_dir, device_id, batch_size=5000, num_imfs=8):
    print(f"Processing device_id: {device_id}")
    device_data = filtered_data[filtered_data['device_id'] == device_id]

    total_rows = device_data.shape[0]
    print(f"Total rows to process for device_id {device_id}: {total_rows}")

    imfs_list = []
    residuals_list = []
    for start in range(0, total_rows, batch_size):
        end = min(start + batch_size, total_rows)
        batch_data = device_data.iloc[start:end]

        print(f"Processing batch from row {start} to {end}...")

        start_time = time.time()

        batch_data, scaler = preprocess_data(batch_data)
        batch_data = detect_and_remove_outliers(batch_data)
        batch_data = impute_missing_values(batch_data)

        imfs, residual = decompose_series(batch_data['W'], save_dir, title=f'Decomposed IMFs for Device {device_id} Batch {start}-{end}', num_imfs=num_imfs)
        imfs_list.append(imfs)
        residuals_list.append(residual)

        end_time = time.time()
        print(f"Batch processed in {end_time - start_time:.2f} seconds.")

    combined_imfs = np.concatenate(imfs_list, axis=1)
    combined_residual = np.concatenate(residuals_list, axis=0)

    # Save combined IMFs and residual
    np.save(os.path.join(save_dir, f'combined_imfs_{device_id}.npy'), combined_imfs)
    np.save(os.path.join(save_dir, f'combined_residual_{device_id}.npy'), combined_residual)

    return combined_imfs, combined_residual

# Run the updated workflow for one device_id
final_sample, save_dir, device_ids = load_and_sample_data()

# Choose one device_id to process
device_id = device_ids[0]
combined_imfs, combined_residual = preprocess_and_decompose_one_device(final_sample, save_dir, device_id, batch_size=5000, num_imfs=8)

import numpy as np
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Function to clean and check the original data
def clean_and_check_data(data):
    # Check for NaNs or infinities in the data
    if np.any(np.isnan(data['W'])):
        print("NaN values found in original data.")
    if np.any(np.isinf(data['W'])):
        print("Infinite values found in original data.")

    # Check the range of the original data
    print("Original data stats:")
    print(data['W'].describe())

    return data

# Function to generate and save the correlation heatmap
def clean_correlation_heatmap(imfs, residual, original_data, save_dir):
    # Prepare the data for the heatmap
    imf_columns = [f'IMF_{i+1}' for i in range(imfs.shape[0])]
    imf_df = pd.DataFrame(imfs.T, columns=imf_columns)
    residual_df = pd.DataFrame(residual, columns=['Residual'])
    original_data = original_data.iloc[:len(imf_df), :]

    # Combine the data
    combined_data = pd.concat([original_data.reset_index(drop=True), imf_df, residual_df], axis=1)

    relevant_columns = ['W'] + imf_columns + ['Residual']
    combined_data = combined_data[relevant_columns]

    corr_matrix = combined_data.corr()

    plt.figure(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1,
                annot_kws={"size": 10}, cbar_kws={"shrink": 0.75})
    plt.title('Correlation Heatmap (Including IMFs and Residual)', fontsize=14)
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.yticks(rotation=0, fontsize=12)
    plt.tight_layout()

    # Save the figure
    plt.savefig(os.path.join(save_dir, 'clean_correlation_heatmap.png'))
    plt.show()
    print("Clean correlation heatmap saved as clean_correlation_heatmap.png.")

# Function to plot IMFs and residuals
def plot_imfs_and_residual(imfs, residual, original_data, save_dir):
    original_data = clean_and_check_data(original_data)

    plt.figure(figsize=(12, 18))
    num_imfs = len(imfs)

    for i, imf in enumerate(imfs):
        plt.subplot(num_imfs + 2, 1, i + 1)
        plt.plot(imf, color='blue')
        plt.title(f'IMF {i + 1}')
        plt.tight_layout()

    # Plot the original data
    plt.subplot(num_imfs + 2, 1, num_imfs + 1)
    plt.plot(original_data['W'].values[:len(imfs[0])], color='orange')
    plt.title('Original Data')
    plt.tight_layout()

    # Plot the residual
    plt.subplot(num_imfs + 2, 1, num_imfs + 2)
    plt.plot(residual, color='red')
    plt.title('Residual')
    plt.tight_layout()

    plt.savefig(os.path.join(save_dir, 'imfs_and_residual.png'))
    plt.show()
    print("IMFs and residual plotted and saved as imfs_and_residual.png.")

    # Generate the correlation heatmap
    clean_correlation_heatmap(imfs, residual, original_data, save_dir)

# Load the saved combined_imfs and combined_residual
save_dir = '/content/drive/MyDrive/thesis/'
device_id = 155

# Load the saved files
combined_imfs = np.load(os.path.join(save_dir, f'combined_imfs_{device_id}.npy'))
combined_residual = np.load(os.path.join(save_dir, f'combined_residual_{device_id}.npy'))


# Example of how to run the function with both IMF and heatmap plotting
plot_imfs_and_residual(combined_imfs, combined_residual, final_sample[['W']], save_dir)

import os
import shutil
import tensorflow as tf

# Step 1: Clear the Tuner Directory
device_id = 155
save_dir = '/content/drive/MyDrive/'
tuner_directory = os.path.join(save_dir, f'device_{device_id}', f'gru_hyperparameter_tuning_{timestamp}')

if os.path.exists(tuner_directory):
    shutil.rmtree(tuner_directory)
    print(f"Removed old tuner directory: {tuner_directory}")
else:
    print(f"No tuner directory found at: {tuner_directory}")

# Step 2: Clear TensorFlow's Session
tf.keras.backend.clear_session()
print("Cleared TensorFlow session.")

!pip show tensorflow

!pip uninstall tensorflow keras keras-tuner -y
!pip install tensorflow keras keras-tuner

!pip install tensorflow-addons

!pip uninstall tf-keras -y

!pip install --upgrade keras-tcn

import numpy as np
import os
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense
from tensorflow.keras.optimizers import Adam
import keras_tuner as kt
from sklearn.model_selection import train_test_split
from tcn import compiled_tcn


def create_dataset(data, look_back=10):
    X, y = [], []
    for i in range(len(data) - look_back - 1):
        X.append(data[i:(i + look_back)])
        y.append(data[i + look_back])
    return np.array(X), np.array(y)


# Function to train models for each device
def train_models_for_each_device(imfs_dict, residuals_dict, save_dir, device_ids, look_back=10):
    models_dict = {}
    for device_id in device_ids:
        print(f"Training models for device_id: {device_id}")
        combined_data = np.vstack((imfs_dict[device_id], residuals_dict[device_id]))
        X, y = create_dataset(combined_data[0], look_back=look_back)
        X = X.reshape(X.shape[0], X.shape[1], 1)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        models = {}
        models['GRU'], models['TCN'] = train_models(X_train, y_train, X_test, y_test, save_dir, device_id)
        models_dict[device_id] = models

    return models_dict

def train_gru_model(X_train, y_train, X_test, y_test, save_dir, device_id):
    print("Starting GRU model training...")

    def build_gru_model(hp):
        model = Sequential()
        hp_units = hp.Int('units', min_value=32, max_value=128, step=32)
        model.add(GRU(units=hp_units, input_shape=(X_train.shape[1], 1)))
        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
        model.add(Dense(1))
        model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss='mean_squared_error')
        return model

    tuner = kt.RandomSearch(
        build_gru_model,
        objective='val_loss',
        max_trials=5,
        executions_per_trial=3,
        directory=os.path.join(save_dir, f'device_{device_id}'),
        project_name='gru_hyperparameter_tuning'
    )

    tuner.search(X_train, y_train, epochs=20, validation_data=(X_test, y_test), batch_size=64)
    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
    best_gru_model = tuner.hypermodel.build(best_hps)
    best_gru_model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), batch_size=64)
    best_gru_model.save(os.path.join(save_dir, f'best_gru_model_device_{device_id}.keras'))
    print(f"GRU model training and tuning completed for device_id: {device_id}.")

    return best_gru_model

def train_tcn_model(X_train, y_train, X_test, y_test, save_dir, device_id, learning_rate=1e-3):
    print("Starting TCN model training...")

    # TCN Model Training
    tcn_model = compiled_tcn(
        num_feat=1,
        num_classes=0,
        nb_filters=20,
        kernel_size=6,
        dilations=[1, 2, 4, 8],
        nb_stacks=1,
        max_len=X_train.shape[1],
        output_len=1,
        regression=True
    )

    tcn_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')
    tcn_model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=1)
    tcn_model.save(os.path.join(save_dir, f'tcn_model_device_{device_id}.keras'))
    print(f"TCN model training completed for device_id: {device_id}.")

    return tcn_model

def train_models_for_each_device_separately(imfs_dict, residuals_dict, save_dir, device_ids, look_back=10):
    models_dict = {}
    for device_id in device_ids:
        print(f"Training models for device_id: {device_id}")

        combined_data = np.vstack((imfs_dict[device_id], residuals_dict[device_id]))
        X, y = create_dataset(combined_data[0], look_back=look_back)
        X = X.reshape(X.shape[0], X.shape[1], 1)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        gru_model = train_gru_model(X_train, y_train, X_test, y_test, save_dir, device_id)
        tcn_model = train_tcn_model(X_train, y_train, X_test, y_test, save_dir, device_id, learning_rate=gru_model.optimizer.learning_rate.numpy())
        models_dict[device_id] = {'GRU': gru_model, 'TCN': tcn_model}

    return models_dict

models_dict = train_models_for_each_device_separately(imfs_dict, residuals_dict, save_dir, device_ids)

def evaluate_and_plot_for_each_device(models_dict, imfs_dict, save_dir, device_ids):
    for device_id in device_ids:
        print(f"Evaluating models for device_id: {device_id}")
        imfs = imfs_dict[device_id]

        X, y = create_dataset(imfs[0], look_back=10)
        X = X.reshape(X.shape[0], X.shape[1], 1)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        models = models_dict[device_id]
        results = evaluate_models(models, X_test, y_test)

        predictions = {name: model.predict(X_test) for name, model in models.items()}
        plot_predictions(predictions, y_test, save_dir)
        print(f"Completed evaluation and plotting for device_id: {device_id}")

def evaluate_models(models, X_test, y_test):
    print("Evaluating models...")
    results = {}
    for name, model in models.items():
        predictions = model.predict(X_test)
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        results[name] = rmse
        print(f"{name} RMSE: {rmse}")
    print("Model evaluation completed.")
    return results

def plot_predictions(predictions, y_test, save_dir):
    print("Plotting predictions...")
    plt.figure(figsize=(12, 6))
    for model_name, pred in predictions.items():
        plt.plot(pred, label=f'{model_name} Predictions', linestyle='--')
    plt.plot(y_test, label='True Values', color='blue')
    plt.title('Model Predictions vs True Values')
    plt.xlabel('Time Steps')
    plt.ylabel('Normalized Power Output')
    plt.legend()
    plt.savefig(os.path.join(save_dir, 'predictions_comparison.png'))
    plt.show()
    print("Prediction plots saved.")

evaluate_and_plot_for_each_device(models_dict, imfs_dict, save_dir, device_ids)

!pip install keras-tcn==3.4.0

import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error

def create_multistep_dataset(data, look_back=10, n_steps=1):
    X, y = [], []
    for i in range(len(data) - look_back - n_steps + 1):
        X.append(data[i:(i + look_back)])
        y.append(data[(i + look_back):(i + look_back + n_steps)])
    return np.array(X), np.array(y)

def evaluate_and_plot_gru_multistep(save_dir, imfs_dict, device_ids, look_back=10, n_steps=3):
    for device_id in device_ids:
        print(f"Evaluating GRU model for device_id: {device_id}")

        gru_model_path = os.path.join(save_dir, f'best_gru_model_device_{device_id}.keras')
        if not os.path.exists(gru_model_path):
            print(f"GRU model file not found for device_id {device_id}. Ensure the model has been saved.")
            continue

        gru_model = load_model(gru_model_path)

        imfs = imfs_dict[device_id]
        X, y = create_multistep_dataset(imfs[0], look_back=look_back, n_steps=n_steps)
        X = X.reshape(X.shape[0], X.shape[1], 1), X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        for step in range(1, n_steps + 1):
            predictions = gru_model.predict(X_test)[:, step - 1]
            y_true = y_test[:, step - 1]

            mae = mean_absolute_error(y_true, predictions)
            rmse = np.sqrt(mean_squared_error(y_true, predictions))
            n_rmse = rmse / np.mean(y_true) * 100

            print(f"\nStep {step} results for device_id {device_id}:")
            print(f"MAE: {mae:.2f} W")
            print(f"RMSE: {rmse:.2f} W")
            print(f"n-RMSE: {n_rmse:.2f} %")

            plot_gru_predictions(predictions, y_true, save_dir, device_id, step)

        print(f"Completed evaluation and plotting for GRU model of device_id: {device_id}")

def plot_gru_predictions(predictions, y_test, save_dir, device_id, step):
    print(f"Plotting GRU predictions for step {step}...")
    plt.figure(figsize=(12, 6))
    plt.plot(predictions, label=f'GRU Predictions (Step {step})', linestyle='--', color='red')
    plt.plot(y_test, label='True Values', color='blue')
    plt.title(f'GRU Predictions vs True Values for Step {step}, Device ID: {device_id}')
    plt.xlabel('Time Steps')
    plt.ylabel('Normalized Power Output')
    plt.legend()
    plt.savefig(os.path.join(save_dir, f'gru_predictions_comparison_device_{device_id}_step_{step}.png'))
    plt.show()
    print(f"GRU prediction plot saved for step {step}, device_id {device_id}.")

evaluate_and_plot_gru_multistep(save_dir, imfs_dict, device_ids)

import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error
from keras.models import load_model

def create_multistep_dataset(data, look_back=10, n_steps=1):
    X, y = [], []
    for i in range(len(data) - look_back - n_steps + 1):
        X.append(data[i:(i + look_back)])
        y.append(data[(i + look_back):(i + look_back + n_steps)])
    return np.array(X), np.array(y)

def evaluate_and_plot_gru_multistep(save_dir, imfs_dict, device_ids, look_back=10, n_steps=3, scaler=None):
    for device_id in device_ids:
        print(f"Evaluating GRU model for device_id: {device_id}")

        gru_model_path = os.path.join(save_dir, f'best_gru_model_device_{device_id}.keras')
        if not os.path.exists(gru_model_path):
            print(f"GRU model file not found for device_id {device_id}. Ensure the model has been saved.")
            continue
        gru_model = load_model(gru_model_path)
        imfs = imfs_dict[device_id]
        X, y = create_multistep_dataset(imfs[0], look_back=look_back, n_steps=n_steps)
        X = X.reshape(X.shape[0], X.shape[1], 1)

        _, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        if scaler is not None:
            y_test = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)
        for step in range(1, n_steps + 1):
            predictions = gru_model.predict(X_test)[:, step - 1]
            if scaler is not None:
                predictions = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()

            y_true = y_test[:, step - 1]

            mae = mean_absolute_error(y_true, predictions)
            rmse = np.sqrt(mean_squared_error(y_true, predictions))
            n_rmse = (rmse / (y_true.max() - y_true.min())) * 100

            print(f"\nStep {step} results for device_id {device_id}:")
            print(f"MAE: {mae:.2f} W")
            print(f"RMSE: {rmse:.2f} W")
            print(f"n-RMSE: {n_rmse:.2f} %")

            plot_gru_predictions(predictions, y_true, save_dir, device_id, step)

        print(f"Completed evaluation and plotting for GRU model of device_id: {device_id}")

def plot_gru_predictions(predictions, y_test, save_dir, device_id, step):
    print(f"Plotting GRU predictions for step {step}...")
    plt.figure(figsize=(12, 6))
    plt.plot(predictions, label=f'GRU Predictions (Step {step})', linestyle='--', color='red')
    plt.plot(y_test, label='True Values', color='blue')
    plt.title(f'GRU Predictions vs True Values for Step {step}, Device ID: {device_id}')
    plt.xlabel('Time Steps')
    plt.ylabel('Power Output (W)')
    plt.legend()
    plt.savefig(os.path.join(save_dir, f'gru_predictions_comparison_device_{device_id}_step_{step}.png'))
    plt.show()
    print(f"GRU prediction plot saved for step {step}, device_id {device_id}.")


evaluate_and_plot_gru_multistep(save_dir, imfs_dict, device_ids, look_back=10, n_steps=3, scaler=scaler)

!pip install catboost

import numpy as np
import os
from catboost import CatBoostRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
from keras.models import Sequential
from keras.layers import Conv1D, Dense, Flatten, BatchNormalization, Dropout
from sklearn.model_selection import train_test_split

# Load the precomputed IMFs for Device 155

combined_imfs_155 = np.load('/content/drive/MyDrive/thesis/combined_imfs_155.npy')

X_155 = combined_imfs_155.T
y_155 = X_155[:, 0]

# Split the data into training and testing sets
X_train_155, X_test_155, y_train_155, y_test_155 = train_test_split(X_155, y_155, test_size=0.2, random_state=42)

# Reshape for TCN model input
X_train_155_tcn = X_train_155.reshape(X_train_155.shape[0], X_train_155.shape[1], 1)
X_test_155_tcn = X_test_155.reshape(X_test_155.shape[0], X_test_155.shape[1], 1)

# Train CatBoost on IMFs
def train_catboost(X_train, y_train, X_test):
    model = CatBoostRegressor(iterations=1000, depth=6, learning_rate=0.1, loss_function='RMSE')
    model.fit(X_train, y_train, verbose=100)
    predictions = model.predict(X_test)
    return predictions, model

# Train TCN on IMFs
def train_tcn(X_train, y_train, X_test, input_shape):
    model = Sequential()
    model.add(Conv1D(64, 2, padding='causal', activation='relu', input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))
    model.add(Conv1D(64, 2, padding='causal', activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(50, activation='relu'))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')

    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)
    predictions = model.predict(X_test).flatten()
    return predictions, model

catboost_pred_155, catboost_model_155 = train_catboost(X_train_155, y_train_155, X_test_155)

tcn_pred_155, tcn_model_155 = train_tcn(X_train_155_tcn, y_train_155, X_test_155_tcn, (X_train_155_tcn.shape[1], 1))

ensemble_pred_155 = (catboost_pred_155 + tcn_pred_155) / 2

mae_ensemble = mean_absolute_error(y_test_155, ensemble_pred_155)
rmse_ensemble = np.sqrt(mean_squared_error(y_test_155, ensemble_pred_155))
n_rmse_ensemble = (rmse_ensemble / (y_test_155.max() - y_test_155.min())) * 100

print(f"Ensemble Model Results for Device 155:")
print(f"MAE: {mae_ensemble:.2f} W")
print(f"RMSE: {rmse_ensemble:.2f} W")
print(f"n-RMSE: {n_rmse_ensemble:.2f} %")

from sklearn.preprocessing import StandardScaler

target_scaler = StandardScaler()

target_scaler.fit(y_train_155.reshape(-1, 1))

y_test_155_orig = target_scaler.inverse_transform(y_test_155.reshape(-1, 1)).flatten()
ensemble_pred_155_orig = target_scaler.inverse_transform(ensemble_pred_155.reshape(-1, 1)).flatten()

mae = mean_absolute_error(y_test_155_orig, ensemble_pred_155_orig)
rmse = np.sqrt(mean_squared_error(y_test_155_orig, ensemble_pred_155_orig))
n_rmse = (rmse / (y_test_155_orig.max() - y_test_155_orig.min())) * 100

print(f"MAE: {mae:.2f} W")
print(f"RMSE: {rmse:.2f} W")
print(f"n-RMSE: {n_rmse:.2f} %")

y_test_155_orig = (y_test_155 * scaler.scale_[0]) + scaler.mean_[0]
ensemble_pred_155_orig = (ensemble_pred_155 * scaler.scale_[0]) + scaler.mean_[0]

mae = mean_absolute_error(y_test_155_orig, ensemble_pred_155_orig)
rmse = np.sqrt(mean_squared_error(y_test_155_orig, ensemble_pred_155_orig))
n_rmse = (rmse / (y_test_155_orig.max() - y_test_155_orig.min())) * 100

print(f"MAE: {mae:.2f} W")
print(f"RMSE: {rmse:.2f} W")
print(f"n-RMSE: {n_rmse:.2f} %")

print("True values:", y_test_155_orig[:10])
print("Predictions:", ensemble_pred_155_orig[:10])

from sklearn.preprocessing import StandardScaler

target_scaler = StandardScaler()
target_scaler.fit(y_train_155.reshape(-1, 1))

y_test_orig = target_scaler.inverse_transform(y_test_155.reshape(-1, 1)).flatten()
ensemble_pred_orig = target_scaler.inverse_transform(ensemble_pred_155.reshape(-1, 1)).flatten()

mae = mean_absolute_error(y_test_orig, ensemble_pred_orig)
rmse = np.sqrt(mean_squared_error(y_test_orig, ensemble_pred_orig))
n_rmse = (rmse / (y_test_orig.max() - y_test_orig.min())) * 100

print(f"MAE: {mae:.2f} W")
print(f"RMSE: {rmse:.2f} W")
print(f"n-RMSE: {n_rmse:.2f} %")

